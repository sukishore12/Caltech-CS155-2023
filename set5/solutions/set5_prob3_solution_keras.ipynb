{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Solution set for CS 155 Set 5, 2019/2020\n",
    "\n",
    "Authors: Suraj Nair, Sid Murching, Alex Cui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from P3CHelpers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D:\n",
    "Fill in the generate_traindata and find_most_similar_pairs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_repr(word_to_index, word):\n",
    "    \"\"\"\n",
    "    Returns one-hot-encoded feature representation of the specified word given\n",
    "    a dictionary mapping words to their one-hot-encoded index.\n",
    "\n",
    "    Arguments:\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        word:          Word whose feature representation we wish to compute.\n",
    "\n",
    "    Returns:\n",
    "        feature_representation:     Feature representation of the passed-in word.\n",
    "    \"\"\"\n",
    "    unique_words = word_to_index.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[word_to_index[word]] = 1\n",
    "    return feature_representation    \n",
    "\n",
    "def generate_traindata(word_list, word_to_index, window_size=4):\n",
    "    \"\"\"\n",
    "    Generates training data for Skipgram model.\n",
    "\n",
    "    Arguments:\n",
    "        word_list:     Sequential list of words (strings).\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        window_size:   Size of Skipgram window. Defaults to 2 \n",
    "                       (use the default value when running your code).\n",
    "\n",
    "    Returns:\n",
    "        (trainX, trainY):     A pair of matrices (trainX, trainY) containing training \n",
    "                              points (one-hot-encoded vectors) and their corresponding labels\n",
    "                              (also one-hot-encoded vectors)\n",
    "\n",
    "    \"\"\"\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    vocab_size = len(word_to_index)\n",
    "    for i in range(len(word_list)):\n",
    "        # Extracts the words at each spot\n",
    "        curr_word = word_list[i]\n",
    "        # Loop over window of words near index i (index of current word)\n",
    "        # Add pairs (x = curr_word, y = window_word) to our training data\n",
    "        # for each window_word in the window.\n",
    "        for j in range(1, window_size):\n",
    "            ahead_idx = i + j\n",
    "            behind_idx = i - j\n",
    "            if ahead_idx < len(word_list):\n",
    "                ahead_word = word_list[ahead_idx]\n",
    "                y = get_word_repr(word_to_index, ahead_word)\n",
    "                x = get_word_repr(word_to_index, curr_word)\n",
    "                trainX.append(x)\n",
    "                trainY.append(y)\n",
    "            if behind_idx > 0:\n",
    "                behind_word = word_list[behind_idx]\n",
    "                y = get_word_repr(word_to_index, behind_word)\n",
    "                x = get_word_repr(word_to_index, curr_word)\n",
    "                trainX.append(x)\n",
    "                trainY.append(y)\n",
    "    return np.array(trainX), np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_pairs(filename, num_latent_factors):\n",
    "    \"\"\"\n",
    "    Find the most similar pairs from the word embeddings computed from\n",
    "    a body of text\n",
    "    \n",
    "    Arguments:\n",
    "        filename:           Text file to read and train embeddings from\n",
    "        num_latent_factors: The number of latent factors / the size of the embedding\n",
    "    \"\"\"\n",
    "    # Load in a list of words from the specified file; remove non-alphanumeric characters\n",
    "    # and make all chars lowercase.\n",
    "    sample_text = load_word_list(filename)\n",
    "\n",
    "    # Create word dictionary\n",
    "    word_to_index = generate_onehot_dict(sample_text)\n",
    "    print(\"Textfile contains %s unique words\"%len(word_to_index))\n",
    "    # Create training data\n",
    "    trainX, trainY = generate_traindata(sample_text, word_to_index)\n",
    "    # Build our model\n",
    "    vocab_size = len(word_to_index)\n",
    "    model = Sequential()\n",
    "    # <hidden_layer> contains our latent factors (vector representation of each word)\t\n",
    "    hidden_layer = Dense(num_latent_factors, input_dim = vocab_size)\n",
    "    model.add(hidden_layer)\n",
    "    # <output_layer> transforms the outputs of <hidden_layer> into a vector of size <vocab_size>.\n",
    "    output_layer = Dense(vocab_size)\n",
    "    model.add(output_layer)\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Compile and fit our model\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(trainX, trainY, batch_size = 100, nb_epoch = 50, verbose=2)\n",
    "    weights, biases = hidden_layer.get_weights()\n",
    "    print(\"Hidden layer weight matrix shape: \", weights.shape)\n",
    "    output_weights, output_biases = output_layer.get_weights()\n",
    "    print(\"Output layer weight matrix shape: \", output_weights.shape)\n",
    "\n",
    "    # Find and print most similar pairs\n",
    "    similar_pairs = most_similar_pairs(weights, word_to_index)\n",
    "    for pair in similar_pairs[:30]:\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3G:\n",
    "Run the function below and report your results for dr_seuss.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 333s - loss: 5.6554 - acc: 0.0351\n",
      "Epoch 2/50\n",
      " - 1s - loss: 5.2166 - acc: 0.0583\n",
      "Epoch 3/50\n",
      " - 1s - loss: 4.8308 - acc: 0.0588\n",
      "Epoch 4/50\n",
      " - 1s - loss: 4.7709 - acc: 0.0588\n",
      "Epoch 5/50\n",
      " - 1s - loss: 4.7525 - acc: 0.0596\n",
      "Epoch 6/50\n",
      " - 1s - loss: 4.7381 - acc: 0.0602\n",
      "Epoch 7/50\n",
      " - 1s - loss: 4.7249 - acc: 0.0617\n",
      "Epoch 8/50\n",
      " - 1s - loss: 4.7122 - acc: 0.0617\n",
      "Epoch 9/50\n",
      " - 1s - loss: 4.6987 - acc: 0.0623\n",
      "Epoch 10/50\n",
      " - 1s - loss: 4.6841 - acc: 0.0637\n",
      "Epoch 11/50\n",
      " - 1s - loss: 4.6681 - acc: 0.0717\n",
      "Epoch 12/50\n",
      " - 1s - loss: 4.6505 - acc: 0.0690\n",
      "Epoch 13/50\n",
      " - 1s - loss: 4.6313 - acc: 0.0717\n",
      "Epoch 14/50\n",
      " - 1s - loss: 4.6113 - acc: 0.0730\n",
      "Epoch 15/50\n",
      " - 1s - loss: 4.5900 - acc: 0.0747\n",
      "Epoch 16/50\n",
      " - 1s - loss: 4.5681 - acc: 0.0788\n",
      "Epoch 17/50\n",
      " - 1s - loss: 4.5460 - acc: 0.0819\n",
      "Epoch 18/50\n",
      " - 1s - loss: 4.5231 - acc: 0.0841\n",
      "Epoch 19/50\n",
      " - 1s - loss: 4.5006 - acc: 0.0848\n",
      "Epoch 20/50\n",
      " - 1s - loss: 4.4779 - acc: 0.0899\n",
      "Epoch 21/50\n",
      " - 1s - loss: 4.4555 - acc: 0.0924\n",
      "Epoch 22/50\n",
      " - 1s - loss: 4.4337 - acc: 0.0942\n",
      "Epoch 23/50\n",
      " - 1s - loss: 4.4122 - acc: 0.0949\n",
      "Epoch 24/50\n",
      " - 1s - loss: 4.3912 - acc: 0.0961\n",
      "Epoch 25/50\n",
      " - 1s - loss: 4.3710 - acc: 0.0983\n",
      "Epoch 26/50\n",
      " - 1s - loss: 4.3513 - acc: 0.0988\n",
      "Epoch 27/50\n",
      " - 1s - loss: 4.3320 - acc: 0.1000\n",
      "Epoch 28/50\n",
      " - 1s - loss: 4.3138 - acc: 0.1021\n",
      "Epoch 29/50\n",
      " - 1s - loss: 4.2957 - acc: 0.1027\n",
      "Epoch 30/50\n",
      " - 1s - loss: 4.2787 - acc: 0.1039\n",
      "Epoch 31/50\n",
      " - 1s - loss: 4.2621 - acc: 0.1048\n",
      "Epoch 32/50\n",
      " - 1s - loss: 4.2462 - acc: 0.1048\n",
      "Epoch 33/50\n",
      " - 1s - loss: 4.2309 - acc: 0.1057\n",
      "Epoch 34/50\n",
      " - 1s - loss: 4.2160 - acc: 0.1041\n",
      "Epoch 35/50\n",
      " - 1s - loss: 4.2018 - acc: 0.1055\n",
      "Epoch 36/50\n",
      " - 1s - loss: 4.1883 - acc: 0.1060\n",
      "Epoch 37/50\n",
      " - 1s - loss: 4.1751 - acc: 0.1058\n",
      "Epoch 38/50\n",
      " - 1s - loss: 4.1626 - acc: 0.1064\n",
      "Epoch 39/50\n",
      " - 1s - loss: 4.1502 - acc: 0.1060\n",
      "Epoch 40/50\n",
      " - 1s - loss: 4.1385 - acc: 0.1067\n",
      "Epoch 41/50\n",
      " - 1s - loss: 4.1274 - acc: 0.1054\n",
      "Epoch 42/50\n",
      " - 1s - loss: 4.1165 - acc: 0.1089\n",
      "Epoch 43/50\n",
      " - 1s - loss: 4.1059 - acc: 0.1087\n",
      "Epoch 44/50\n",
      " - 1s - loss: 4.0955 - acc: 0.1070\n",
      "Epoch 45/50\n",
      " - 1s - loss: 4.0859 - acc: 0.1092\n",
      "Epoch 46/50\n",
      " - 1s - loss: 4.0763 - acc: 0.1093\n",
      "Epoch 47/50\n",
      " - 1s - loss: 4.0672 - acc: 0.1085\n",
      "Epoch 48/50\n",
      " - 1s - loss: 4.0583 - acc: 0.1101\n",
      "Epoch 49/50\n",
      " - 1s - loss: 4.0500 - acc: 0.1101\n",
      "Epoch 50/50\n",
      " - 1s - loss: 4.0417 - acc: 0.1101\n",
      "Hidden layer weight matrix shape:  (308, 10)\n",
      "Output layer weight matrix shape:  (10, 308)\n",
      "Pair(today, gone), Similarity: 0.9823166\n",
      "Pair(gone, today), Similarity: 0.9823166\n",
      "Pair(top, finger), Similarity: 0.980066\n",
      "Pair(finger, top), Similarity: 0.980066\n",
      "Pair(eight, nine), Similarity: 0.97925264\n",
      "Pair(nine, eight), Similarity: 0.97925264\n",
      "Pair(more, sad), Similarity: 0.9752632\n",
      "Pair(sad, more), Similarity: 0.9752632\n",
      "Pair(likes, thing), Similarity: 0.9752325\n",
      "Pair(thing, likes), Similarity: 0.9752325\n",
      "Pair(drink, thing), Similarity: 0.97164893\n",
      "Pair(fox, mouse), Similarity: 0.9669468\n",
      "Pair(mouse, fox), Similarity: 0.9669468\n",
      "Pair(pink, likes), Similarity: 0.9650593\n",
      "Pair(cold, off), Similarity: 0.9618965\n",
      "Pair(off, cold), Similarity: 0.9618965\n",
      "Pair(foot, shoe), Similarity: 0.9604066\n",
      "Pair(shoe, foot), Similarity: 0.9604066\n",
      "Pair(fish, black), Similarity: 0.960312\n",
      "Pair(black, fish), Similarity: 0.960312\n",
      "Pair(boat, goat), Similarity: 0.96024203\n",
      "Pair(goat, boat), Similarity: 0.96024203\n",
      "Pair(wink, likes), Similarity: 0.957509\n",
      "Pair(ink, pink), Similarity: 0.9524351\n",
      "Pair(five, seven), Similarity: 0.9514447\n",
      "Pair(seven, five), Similarity: 0.9514447\n",
      "Pair(thin, slow), Similarity: 0.95058876\n",
      "Pair(slow, thin), Similarity: 0.95058876\n",
      "Pair(goodbye, wire), Similarity: 0.9495902\n",
      "Pair(wire, goodbye), Similarity: 0.9495902\n"
     ]
    }
   ],
   "source": [
    "find_most_similar_pairs('data/dr_seuss.txt', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Outputs:\\nHidden layer weight matrix shape:  (308, 10)\\nOutput layer weight matrix shape:  (10, 308)\\nPair(today, tomorrow), Similarity: 0.98951316\\nPair(tomorrow, today), Similarity: 0.98951316\\nPair(finger, top), Similarity: 0.9871409\\nPair(top, finger), Similarity: 0.9871409\\nPair(gone, today), Similarity: 0.9847697\\nPair(fox, goat), Similarity: 0.98089796\\nPair(goat, fox), Similarity: 0.98089796\\nPair(heads, grows), Similarity: 0.9799606\\nPair(grows, heads), Similarity: 0.9799606\\nPair(shoe, foot), Similarity: 0.97770756\\nPair(foot, shoe), Similarity: 0.97770756\\nPair(off, shoe), Similarity: 0.9769319\\nPair(zeep, tomorrow), Similarity: 0.9766475\\nPair(likes, wink), Similarity: 0.97186935\\nPair(wink, likes), Similarity: 0.97186935\\nPair(drink, wink), Similarity: 0.9711438\\nPair(his, book), Similarity: 0.96769226\\nPair(book, his), Similarity: 0.96769226\\nPair(hop, then), Similarity: 0.96711636\\nPair(then, hop), Similarity: 0.96711636\\nPair(eight, nine), Similarity: 0.9662633\\nPair(nine, eight), Similarity: 0.9662633\\nPair(mouse, fox), Similarity: 0.96473193\\nPair(these, pets), Similarity: 0.9645774\\nPair(pets, these), Similarity: 0.9645774\\nPair(cannot, hear), Similarity: 0.96193874\\nPair(hear, cannot), Similarity: 0.96193874\\nPair(ride, fly), Similarity: 0.9618145\\nPair(fly, ride), Similarity: 0.9618145\\nPair(thing, drink), Similarity: 0.96120024\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Outputs:\n",
    "Hidden layer weight matrix shape:  (308, 10)\n",
    "Output layer weight matrix shape:  (10, 308)\n",
    "Pair(today, tomorrow), Similarity: 0.98951316\n",
    "Pair(tomorrow, today), Similarity: 0.98951316\n",
    "Pair(finger, top), Similarity: 0.9871409\n",
    "Pair(top, finger), Similarity: 0.9871409\n",
    "Pair(gone, today), Similarity: 0.9847697\n",
    "Pair(fox, goat), Similarity: 0.98089796\n",
    "Pair(goat, fox), Similarity: 0.98089796\n",
    "Pair(heads, grows), Similarity: 0.9799606\n",
    "Pair(grows, heads), Similarity: 0.9799606\n",
    "Pair(shoe, foot), Similarity: 0.97770756\n",
    "Pair(foot, shoe), Similarity: 0.97770756\n",
    "Pair(off, shoe), Similarity: 0.9769319\n",
    "Pair(zeep, tomorrow), Similarity: 0.9766475\n",
    "Pair(likes, wink), Similarity: 0.97186935\n",
    "Pair(wink, likes), Similarity: 0.97186935\n",
    "Pair(drink, wink), Similarity: 0.9711438\n",
    "Pair(his, book), Similarity: 0.96769226\n",
    "Pair(book, his), Similarity: 0.96769226\n",
    "Pair(hop, then), Similarity: 0.96711636\n",
    "Pair(then, hop), Similarity: 0.96711636\n",
    "Pair(eight, nine), Similarity: 0.9662633\n",
    "Pair(nine, eight), Similarity: 0.9662633\n",
    "Pair(mouse, fox), Similarity: 0.96473193\n",
    "Pair(these, pets), Similarity: 0.9645774\n",
    "Pair(pets, these), Similarity: 0.9645774\n",
    "Pair(cannot, hear), Similarity: 0.96193874\n",
    "Pair(hear, cannot), Similarity: 0.96193874\n",
    "Pair(ride, fly), Similarity: 0.9618145\n",
    "Pair(fly, ride), Similarity: 0.9618145\n",
    "Pair(thing, drink), Similarity: 0.96120024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
