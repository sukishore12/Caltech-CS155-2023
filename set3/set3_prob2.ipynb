{
 "cells": [
  {
    "cell_type": "markdown",
    "source": [
        "<a href=\"https://githubtocolab.com/emiletimothy/Caltech-CS155-2023/blob/main/set3/set3_prob2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
     ],
    "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements and function to load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and divide it into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number 24 in the next line corresponds to the number of header lines\n",
    "X = load_data('https://raw.githubusercontent.com/emiletimothy/Caltech-CS155-2023/main/set3/data/messidor_features.arff', 24)\n",
    "\n",
    "data = X[:, :-1]\n",
    "diag = X[:, -1]\n",
    "\n",
    "train_size = 900\n",
    "\n",
    "train_data = data[0:train_size]\n",
    "train_label = diag[0:train_size]\n",
    "test_data = data[train_size:]\n",
    "test_label = diag[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2A: Decision Trees with Minimum Leaf Size Stopping Criterion\n",
    "\n",
    "Fill in the two functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_err(y, real_y):\n",
    "    \"\"\"\n",
    "    This function returns the classification error between two equally-sized vectors of \n",
    "    labels; this is the fraction of samples for which the labels differ.\n",
    "    \n",
    "    Inputs:\n",
    "        y: (N, ) shaped array of predicted labels\n",
    "        real_y: (N, ) shaped array of true labels\n",
    "    Output:\n",
    "        Scalar classification error\n",
    "    \"\"\"\n",
    "    #==============================================\n",
    "    # TODO: Implement the classification_err function,\n",
    "    # based on the above instructions.\n",
    "    #==============================================    \n",
    "    pass\n",
    "\n",
    "def eval_tree_based_model_min_samples(clf, min_samples_leaf, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function evaluates the given classifier (either a decision tree or random forest) at all of the \n",
    "    minimum leaf size parameters in the vector min_samples_leaf, using the given training and testing\n",
    "    data. It returns two vector, with the training and testing classification errors.\n",
    "    \n",
    "    Inputs:\n",
    "        clf: either a decision tree or random forest classifier object\n",
    "        min_samples_leaf: a (T, ) vector of all the min_samples_leaf stopping condition parameters \n",
    "                            to test, where T is the number of parameters to test\n",
    "        X_train: (N, D) matrix of training samples.\n",
    "        y_train: (N, ) vector of training labels.\n",
    "        X_test: (N, D) matrix of test samples\n",
    "        y_test: (N, ) vector of test labels\n",
    "    Output:\n",
    "        train_err: (T, ) vector of classification errors on the training data\n",
    "        test_err: (T, ) vector of classification errors on the test data\n",
    "    \"\"\"\n",
    "    #================================================================\n",
    "    # TODO: Implement the eval_tree_based_model_min_samples function,\n",
    "    # based on the above instructions.\n",
    "    #================================================================ \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "min_samples_leaf = np.arange(1, 26)\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "train_err, test_err = eval_tree_based_model_min_samples(clf, min_samples_leaf, train_data, \n",
    "                                                        train_label, test_data, test_label)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(min_samples_leaf, test_err, label='Testing error')\n",
    "plt.plot(min_samples_leaf, train_err, label='Training error')\n",
    "plt.xlabel('Minimum Node Size')\n",
    "plt.ylabel('Classification error')\n",
    "plt.title('Decision Tree with Gini Impurity and Minimum Node Size')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "print('Test error minimized at min_samples_leaf = %i' % min_samples_leaf[np.argmin(test_err)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2B: Decision Trees with Maximum Depth Stopping Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_tree_based_model_max_depth(clf, max_depth, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function evaluates the given classifier (either a decision tree or random forest) at all of the \n",
    "    maximum tree depth parameters in the vector max_depth, using the given training and testing\n",
    "    data. It returns two vector, with the training and testing classification errors.\n",
    "    \n",
    "    Inputs:\n",
    "        clf: either a decision tree or random forest classifier object\n",
    "        max_depth: a (T, ) vector of all the max_depth stopping condition parameters \n",
    "                            to test, where T is the number of parameters to test\n",
    "        X_train: (N, D) matrix of training samples.\n",
    "        y_train: (N, ) vector of training labels.\n",
    "        X_test: (N, D) matrix of test samples\n",
    "        y_test: (N, ) vector of test labels\n",
    "    Output:\n",
    "        train_err: (T, ) vector of classification errors on the training data\n",
    "        test_err: (T, ) vector of classification errors on the test data\n",
    "    \"\"\"\n",
    "    #================================================================\n",
    "    # TODO: Implement the eval_tree_based_model_max_depth function,\n",
    "    # based on the above instructions.\n",
    "    #================================================================ \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "max_depth = np.arange(2, 21)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "train_err, test_err = eval_tree_based_model_max_depth(clf, max_depth, train_data, \n",
    "                                                        train_label, test_data, test_label)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(max_depth, test_err, label='Testing error')\n",
    "plt.plot(max_depth, train_err, label='Training error')\n",
    "plt.xlabel('Maximum Tree Depth')\n",
    "plt.ylabel('Classification error')\n",
    "plt.title('Decision Tree with Gini Impurity and Maximum Tree Depth')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "print('Test error minimized at max_depth = %i' % max_depth[np.argmin(test_err)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2D: Random Forests with Minimum Leaf Size Stopping Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "n_estimators = 1000\n",
    "clf = RandomForestClassifier(n_estimators = n_estimators, criterion = 'gini')\n",
    "\n",
    "min_samples_leaf = np.arange(1, 26)\n",
    "\n",
    "train_err, test_err = eval_tree_based_model_min_samples(clf, min_samples_leaf, train_data, \n",
    "                                                        train_label, test_data, test_label)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(min_samples_leaf, test_err, label='Testing error')\n",
    "plt.plot(min_samples_leaf, train_err, label='Training error')\n",
    "plt.xlabel('Minimum Node Size')\n",
    "plt.ylabel('Classification error')\n",
    "plt.title('Random Forest with Gini Impurity and Minimum Node Size')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "print('Test error minimized at min_samples_leaf = %i' % min_samples_leaf[np.argmin(test_err)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2E: Random Forests with Maximum Depth Stopping Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = n_estimators, criterion = 'gini')\n",
    "\n",
    "max_depth = np.arange(2, 21)\n",
    "\n",
    "train_err, test_err = eval_tree_based_model_max_depth(clf, max_depth, train_data, \n",
    "                                                        train_label, test_data, test_label)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(max_depth, test_err, label='Testing error')\n",
    "plt.plot(max_depth, train_err, label='Training error')\n",
    "plt.xlabel('Maximum Depth')\n",
    "plt.ylabel('Classification error')\n",
    "plt.title('Random Forest with Gini Impurity and Maximum Depth')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "print('Test error minimized at max_depth = %i' % max_depth[np.argmin(test_err)])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11 (main, Mar 22 2022, 16:46:40) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "51cdd729484e546d32179e58cfece8a45dfa365797bcf0655ab12c854818ccd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
