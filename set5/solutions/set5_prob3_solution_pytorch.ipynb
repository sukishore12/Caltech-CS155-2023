{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Solution set for CS 155 Set 5, 2019/2020\n",
    "\n",
    "Authors: Suraj Nair, Sid Murching, Alex Cui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from P3CHelpers import *\n",
    "import sys\n",
    "# load data into PyTorch format\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D:\n",
    "Fill in the generate_traindata and find_most_similar_pairs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_repr(word_to_index, word):\n",
    "    \"\"\"\n",
    "    Returns one-hot-encoded feature representation of the specified word given\n",
    "    a dictionary mapping words to their one-hot-encoded index.\n",
    "\n",
    "    Arguments:\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        word:          Word whose feature representation we wish to compute.\n",
    "\n",
    "    Returns:\n",
    "        feature_representation:     Feature representation of the passed-in word.\n",
    "    \"\"\"\n",
    "    unique_words = word_to_index.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[word_to_index[word]] = 1\n",
    "    return feature_representation    \n",
    "\n",
    "def generate_traindata(word_list, word_to_index, window_size=4):\n",
    "    \"\"\"\n",
    "    Generates training data for Skipgram model.\n",
    "\n",
    "    Arguments:\n",
    "        word_list:     Sequential list of words (strings).\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        window_size:   Size of Skipgram window. Defaults to 2 \n",
    "                       (use the default value when running your code).\n",
    "\n",
    "    Returns:\n",
    "        (trainX, trainY):     A pair of matrices (trainX, trainY) containing training \n",
    "                              points (one-hot-encoded vectors) and their corresponding output_word\n",
    "                              (also one-hot-encoded vectors)\n",
    "\n",
    "    \"\"\"\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    vocab_size = len(word_to_index)\n",
    "    for i in range(len(word_list)):\n",
    "        # Extracts the words at each spot\n",
    "        curr_word = word_list[i]\n",
    "        # Loop over window of words near index i (index of current word)\n",
    "        # Add pairs (x = curr_word, y = window_word) to our training data\n",
    "        # for each window_word in the window.\n",
    "        for j in range(1, window_size):\n",
    "            ahead_idx = i + j\n",
    "            behind_idx = i - j\n",
    "            if ahead_idx < len(word_list):\n",
    "                ahead_word = word_list[ahead_idx]\n",
    "                y = get_word_repr(word_to_index, ahead_word)\n",
    "                x = get_word_repr(word_to_index, curr_word)\n",
    "                trainX.append(x)\n",
    "                trainY.append(y)\n",
    "            if behind_idx > 0:\n",
    "                behind_word = word_list[behind_idx]\n",
    "                y = get_word_repr(word_to_index, behind_word)\n",
    "                x = get_word_repr(word_to_index, curr_word)\n",
    "                trainX.append(x)\n",
    "                trainY.append(y)\n",
    "    return np.array(trainX), np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_pairs(filename, num_latent_factors):\n",
    "    \"\"\"\n",
    "    Find the most similar pairs from the word embeddings computed from\n",
    "    a body of text\n",
    "    \n",
    "    Arguments:\n",
    "        filename:           Text file to read and train embeddings from\n",
    "        num_latent_factors: The number of latent factors / the size of the embedding\n",
    "    \"\"\"\n",
    "    # Load in a list of words from the specified file; remove non-alphanumeric characters\n",
    "    # and make all chars lowercase.\n",
    "    sample_text = load_word_list(filename)\n",
    "\n",
    "    # Create word dictionary\n",
    "    word_to_index = generate_onehot_dict(sample_text)\n",
    "    print(\"Textfile contains %s unique words\"%len(word_to_index))\n",
    "    # Create training data\n",
    "    trainX, trainY = generate_traindata(sample_text, word_to_index)\n",
    "    # Build our model\n",
    "    vocab_size = len(word_to_index)\n",
    "\n",
    "    # set batch size\n",
    "    batch_size = 100\n",
    "    \n",
    "    # transform to torch tensor\n",
    "    tensor_x = torch.Tensor(trainX)\n",
    "    # Pytorch expects the label to be a categorical label, not a one-hot vector\n",
    "    # Thus, we convert one-hot to categorical here. In keras, it should be kept\n",
    "    # as a one-hot vector.\n",
    "    # For example, the label: [0, 0, 1, 0] should be converted to [2]\n",
    "    tensor_y = torch.Tensor(np.argmax(trainY, axis=1)).long()\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "    training_data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       shuffle=True)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(vocab_size, num_latent_factors),\n",
    "        nn.Linear(num_latent_factors, vocab_size)\n",
    "        # softmax activation included in cross entropy calculation, so do not\n",
    "        # include it in your model.\n",
    "    )\n",
    "    # our model has some # of parameters:\n",
    "    print('Note: Matrices are the weights,',\n",
    "          'vectors are the bias vectors')\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        print('Layer', i, ' has shape:', p.data.shape, '\\n')\n",
    "\n",
    "    # For a multi-class classification problem\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # set number training epochs\n",
    "    n_epochs = 50\n",
    "    # store metrics for plotting\n",
    "    training_accuracy_history = np.zeros([n_epochs, 1])\n",
    "    training_loss_history = np.zeros([n_epochs, 1])\n",
    "\n",
    "    # Train the model, iterating on the data in batches\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch', (epoch+1), end='')\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        # train\n",
    "        model.train()\n",
    "        for i, data in enumerate(training_data_loader):\n",
    "            input_word, output_word = data\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model(input_word)\n",
    "            # calculate categorical cross entropy loss\n",
    "            loss = criterion(output, output_word)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # track training accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += output_word.size(0)\n",
    "            train_correct += (predicted == output_word).sum().item()\n",
    "            # track training loss\n",
    "            training_loss_history[epoch] += loss.item()\n",
    "        training_loss_history[epoch] /= len(training_data_loader)\n",
    "        training_accuracy_history[epoch] = train_correct / train_total\n",
    "        print('\\n\\tloss: ',training_loss_history[epoch,0], 'acc:', training_accuracy_history[epoch,0])\n",
    "\n",
    "\n",
    "    weights = list(model.parameters())[0].data.numpy().T\n",
    "    print(\"Hidden layer weight matrix shape: \", weights.shape)\n",
    "    output_weights = list(model.parameters())[2].data.numpy().T\n",
    "    print(\"Output layer weight matrix shape: \", output_weights.shape)\n",
    "\n",
    "    # Find and print most similar pairs\n",
    "    similar_pairs = most_similar_pairs(weights, word_to_index)\n",
    "    for pair in similar_pairs[:30]:\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3G:\n",
    "Run the function below and report your results for dr_seuss.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textfile contains 308 unique words\n",
      "Note: Matrices are the weights, vectors are the bias vectors\n",
      "Layer 0  has shape: torch.Size([10, 308]) \n",
      "\n",
      "Layer 1  has shape: torch.Size([10]) \n",
      "\n",
      "Layer 2  has shape: torch.Size([308, 10]) \n",
      "\n",
      "Layer 3  has shape: torch.Size([308]) \n",
      "\n",
      "Epoch 1\n",
      "\tloss:  5.638266773223877 acc: 0.024655547498187092\n",
      "Epoch 2\n",
      "\tloss:  5.212681678771973 acc: 0.03964225284022238\n",
      "Epoch 3\n",
      "\tloss:  4.8604635124206546 acc: 0.05922165820642978\n",
      "Epoch 4\n",
      "\tloss:  4.773799694061279 acc: 0.05897993715252599\n",
      "Epoch 5\n",
      "\tloss:  4.7612561492919925 acc: 0.05897993715252599\n",
      "Epoch 6\n",
      "\tloss:  4.740185813903809 acc: 0.05914108452179518\n",
      "Epoch 7\n",
      "\tloss:  4.7290279579162595 acc: 0.05897993715252599\n",
      "Epoch 8\n",
      "\tloss:  4.707237537384033 acc: 0.06325034243815969\n",
      "Epoch 9\n",
      "\tloss:  4.695675735473633 acc: 0.06486181613085167\n",
      "Epoch 10\n",
      "\tloss:  4.671096858978271 acc: 0.06864877930867778\n",
      "Epoch 11\n",
      "\tloss:  4.651651601791382 acc: 0.06647328982354363\n",
      "Epoch 12\n",
      "\tloss:  4.637347282409668 acc: 0.07662557408750302\n",
      "Epoch 13\n",
      "\tloss:  4.61840571975708 acc: 0.07541696881798404\n",
      "Epoch 14\n",
      "\tloss:  4.590041797637939 acc: 0.07839819514946418\n",
      "Epoch 15\n",
      "\tloss:  4.5705062179565425 acc: 0.07976794778825236\n",
      "Epoch 16\n",
      "\tloss:  4.542746315002441 acc: 0.08170171621948272\n",
      "Epoch 17\n",
      "\tloss:  4.521383037567139 acc: 0.08556925308194344\n",
      "Epoch 18\n",
      "\tloss:  4.496525241851806 acc: 0.08798646362098139\n",
      "Epoch 19\n",
      "\tloss:  4.473783740997314 acc: 0.09048424784465393\n",
      "Epoch 20\n",
      "\tloss:  4.450647953033447 acc: 0.09112883732173073\n",
      "Epoch 21\n",
      "\tloss:  4.426593914031982 acc: 0.09024252679075014\n",
      "Epoch 22\n",
      "\tloss:  4.400955923080445 acc: 0.0922568689066151\n",
      "Epoch 23\n",
      "\tloss:  4.380685312271118 acc: 0.09612440576907581\n",
      "Epoch 24\n",
      "\tloss:  4.365581890106201 acc: 0.09604383208444123\n",
      "Epoch 25\n",
      "\tloss:  4.339808731079102 acc: 0.09652727419224881\n",
      "Epoch 26\n",
      "\tloss:  4.323672431945801 acc: 0.09926677946982515\n",
      "Epoch 27\n",
      "\tloss:  4.303635486602783 acc: 0.10128112158569011\n",
      "Epoch 28\n",
      "\tloss:  4.285752740859985 acc: 0.10095882684715173\n",
      "Epoch 29\n",
      "\tloss:  4.266918495178222 acc: 0.10111997421642091\n",
      "Epoch 30\n",
      "\tloss:  4.254258150100708 acc: 0.10216743211667069\n",
      "Epoch 31\n",
      "\tloss:  4.243058410644531 acc: 0.10313431633228587\n",
      "Epoch 32\n",
      "\tloss:  4.227178127288818 acc: 0.10273144790911289\n",
      "Epoch 33\n",
      "\tloss:  4.206510917663574 acc: 0.10369833212472807\n",
      "Epoch 34\n",
      "\tloss:  4.1992986812591555 acc: 0.10281202159374749\n",
      "Epoch 35\n",
      "\tloss:  4.181166711807251 acc: 0.10410120054790105\n",
      "Epoch 36\n",
      "\tloss:  4.172197202682495 acc: 0.10539037950205463\n",
      "Epoch 37\n",
      "\tloss:  4.1589680061340335 acc: 0.10458464265570865\n",
      "Epoch 38\n",
      "\tloss:  4.143458385467529 acc: 0.10474579002497784\n",
      "Epoch 39\n",
      "\tloss:  4.1304472713470455 acc: 0.10571267424059302\n",
      "Epoch 40\n",
      "\tloss:  4.119553049087524 acc: 0.10410120054790105\n",
      "Epoch 41\n",
      "\tloss:  4.115671766281128 acc: 0.10466521634034325\n",
      "Epoch 42\n",
      "\tloss:  4.102779886245727 acc: 0.10394005317863186\n",
      "Epoch 43\n",
      "\tloss:  4.094923070907592 acc: 0.10329546370155507\n",
      "Epoch 44\n",
      "\tloss:  4.086013959884643 acc: 0.10337603738618967\n",
      "Epoch 45\n",
      "\tloss:  4.072674066543579 acc: 0.10385947949399726\n",
      "Epoch 46\n",
      "\tloss:  4.067169492721558 acc: 0.10466521634034325\n",
      "Epoch 47\n",
      "\tloss:  4.0577929363250735 acc: 0.1068407058254774\n",
      "Epoch 48\n",
      "\tloss:  4.047087594985962 acc: 0.10555152687132383\n",
      "Epoch 49\n",
      "\tloss:  4.037620725631714 acc: 0.10724357424865039\n",
      "Epoch 50\n",
      "\tloss:  4.0347063426971435 acc: 0.10611554266376601\n",
      "Hidden layer weight matrix shape:  (308, 10)\n",
      "Output layer weight matrix shape:  (10, 308)\n",
      "Pair(likes, pink), Similarity: 0.98707074\n",
      "Pair(pink, likes), Similarity: 0.98707074\n",
      "Pair(hook, book), Similarity: 0.9819316\n",
      "Pair(book, hook), Similarity: 0.9819316\n",
      "Pair(sad, thin), Similarity: 0.98086506\n",
      "Pair(thin, sad), Similarity: 0.98086506\n",
      "Pair(finger, top), Similarity: 0.97639173\n",
      "Pair(top, finger), Similarity: 0.97639173\n",
      "Pair(fox, goat), Similarity: 0.97493595\n",
      "Pair(goat, fox), Similarity: 0.97493595\n",
      "Pair(drink, thing), Similarity: 0.9742994\n",
      "Pair(thing, drink), Similarity: 0.9742994\n",
      "Pair(his, book), Similarity: 0.9735982\n",
      "Pair(wink, pink), Similarity: 0.9694164\n",
      "Pair(mr, gump), Similarity: 0.9680575\n",
      "Pair(gump, mr), Similarity: 0.9680575\n",
      "Pair(shoe, off), Similarity: 0.9674432\n",
      "Pair(off, shoe), Similarity: 0.9674432\n",
      "Pair(cant, nook), Similarity: 0.9668873\n",
      "Pair(nook, cant), Similarity: 0.9668873\n",
      "Pair(glad, sad), Similarity: 0.96655375\n",
      "Pair(bad, glad), Similarity: 0.96634287\n",
      "Pair(today, gone), Similarity: 0.96505225\n",
      "Pair(gone, today), Similarity: 0.96505225\n",
      "Pair(left, girls), Similarity: 0.96266896\n",
      "Pair(girls, left), Similarity: 0.96266896\n",
      "Pair(slow, thin), Similarity: 0.95876515\n",
      "Pair(very, thin), Similarity: 0.95461684\n",
      "Pair(foot, shoe), Similarity: 0.9523563\n",
      "Pair(what, bed), Similarity: 0.9515683\n"
     ]
    }
   ],
   "source": [
    "find_most_similar_pairs('data/dr_seuss.txt', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
