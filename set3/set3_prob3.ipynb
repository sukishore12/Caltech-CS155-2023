{
 "cells": [
  {
    "cell_type": "markdown",
    "source": [
        "<a href=\"https://githubtocolab.com/emiletimothy/Caltech-CS155-2023/blob/main/set3/set3_prob3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
     ],
    "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we visualize how boosting and AdaBoost work. This notebook requires FFmpeg; instructions to install it can be found in set 1.\n",
    "\n",
    "Use this notebook to write your code for problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load helper file\n",
    "import os\n",
    "if not os.path.exists('boosting_helper.py'):\n",
    "    os.system('wget https://raw.githubusercontent.com/emiletimothy/Caltech-CS155-2023/main/set3/data/boosting_helper.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from boosting_helper import (\n",
    "    generate_dataset,\n",
    "    visualize_dataset,\n",
    "    gb_suite, ab_suite,\n",
    "    visualize_loss_curves_gb, visualize_loss_curves_ab,\n",
    "    animate_gb, animate_ab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off by generating a complex, slightly noisy 2-dimensional dataset (namely, two spirals) with +1 or -1 as labels. Note that learning on this dataset is a classification problem.\n",
    "\n",
    "(Note: red indicates positive and blue indicates negative.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset with 2000 training points and 500 test points.\n",
    "# Refer to the source code for more details.\n",
    "(X_train, Y_train), (X_test, Y_test) = generate_dataset(2000, 500, 1.5, 4.0)\n",
    "\n",
    "# Visualize the generated dataset.\n",
    "visualize_dataset(X_train, Y_train, 'Training dataset')\n",
    "visualize_dataset(X_test, Y_test, 'Test dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a simple gradient boosting model to classify this dataset. Since we are implementing gradient boosting, we will need to use regressors even though we are dealing with a classification problem. To resolve this issue, we can simply take the sign of the predictions as the predictions of the classifier.\n",
    "\n",
    "Our weak regressors will be decision trees with a maximum of `n_nodes=4` leaf nodes. You can use the following line to create a DT weak regressor:\n",
    "\n",
    "`clf = DecisionTreeRegressor(max_leaf_nodes=n_nodes)`\n",
    "\n",
    "Fill in the `fit()` method in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientBoosting():\n",
    "    def __init__(self, n_clfs=100):\n",
    "        '''\n",
    "        Initialize the gradient boosting model.\n",
    "\n",
    "        Inputs:\n",
    "            n_clfs (default 100): Initializer for self.n_clfs.        \n",
    "                \n",
    "        Attributes:\n",
    "            self.n_clfs: The number of DT weak regressors.\n",
    "            self.clfs: A list of the DT weak regressors, initialized as empty.\n",
    "        '''\n",
    "        self.n_clfs = n_clfs\n",
    "        self.clfs = []\n",
    "        \n",
    "    def fit(self, X, Y, n_nodes=4):\n",
    "        '''\n",
    "        Fit the gradient boosting model. Note that since we are implementing this method in a class,\n",
    "        rather than having a bunch of inputs and outputs, you will deal with the attributes of the class.\n",
    "        (see the __init__() method).\n",
    "        \n",
    "        This method should thus train self.n_clfs DT weak regressors and store them in self.clfs.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "            Y: A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "               (Even though the labels are ints, we treat them as floats.)\n",
    "            n_nodes: The max number of nodes that the DT weak regressors are allowed to have.\n",
    "        '''\n",
    "        #==============================================\n",
    "        # TODO: implement the fit function.\n",
    "        #==============================================   \n",
    "    pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict on the given dataset.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "\n",
    "        Outputs:\n",
    "            A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "            (Even though the labels are ints, we treat them as floats.)\n",
    "        '''\n",
    "        # Initialize predictions.\n",
    "        Y_pred = np.zeros(len(X))\n",
    "        \n",
    "        # Add predictions from each DT weak regressor.\n",
    "        for clf in self.clfs:\n",
    "            Y_curr = clf.predict(X)\n",
    "            Y_pred += Y_curr\n",
    "\n",
    "        # Return the sign of the predictions.\n",
    "        return np.sign(Y_pred)\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Calculate the classification loss.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "            Y: A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "               (Even though the labels are ints, we treat them as floats.)\n",
    "            \n",
    "        Outputs:\n",
    "            The classification loss.\n",
    "        '''\n",
    "        # Calculate the points where the predictions and the ground truths don't match.\n",
    "        Y_pred = self.predict(X)\n",
    "        misclassified = np.where(Y_pred != Y)[0]\n",
    "\n",
    "        # Return the fraction of such points.\n",
    "        return float(len(misclassified)) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the prediction results with 500 weak regressors. Note that misclassified points are marked in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = gb_suite(GradientBoosting, 500, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Gradient Boosting Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the training process of the gradient boosting model. First, we visualize how the predictions change with each new weak regressor that we train and add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = animate_gb(model, X_train, Y_train, 'Training dataset predictions per iteration')\n",
    "display(HTML(anim.to_html5_video()))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize how the loss decreases with each new weak regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_loss_curves_gb(model, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used regression above for a classification problem. A better approach would be to use AdaBoost, which is a natural adaptation for classification.\n",
    "\n",
    "Let's implement an AdaBoost model to classify this dataset. This time, our weak classifiers (not regressors!) will be decision trees with a maximum of `n_nodes=4` leaf nodes. You can use the following line to create a DT weak classifier:\n",
    "\n",
    "`clf = DecisionTreeClassifier(max_leaf_nodes=n_nodes)`\n",
    "\n",
    "Fill in the fit() method in the cell below.\n",
    "\n",
    "**NOTE: Use the 0/1 loss here instead of the exponential loss!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_clfs=100):\n",
    "        '''\n",
    "        Initialize the AdaBoost model.\n",
    "\n",
    "        Inputs:\n",
    "            n_clfs (default 100): Initializer for self.n_clfs.        \n",
    "                \n",
    "        Attributes:\n",
    "            self.n_clfs: The number of DT weak classifiers.\n",
    "            self.coefs: A list of the AdaBoost coefficients.\n",
    "            self.clfs: A list of the DT weak classifiers, initialized as empty.\n",
    "        '''\n",
    "        self.n_clfs = n_clfs\n",
    "        self.coefs = []\n",
    "        self.clfs = []\n",
    "\n",
    "    def fit(self, X, Y, n_nodes=4):\n",
    "        '''\n",
    "        Fit the AdaBoost model. Note that since we are implementing this method in a class, rather\n",
    "        than having a bunch of inputs and outputs, you will deal with the attributes of the class.\n",
    "        (see the __init__() method).\n",
    "        \n",
    "        This method should thus train self.n_clfs DT weak classifiers and store them in self.clfs,\n",
    "        with their coefficients in self.coefs.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "            Y: A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "               (Even though the labels are ints, we treat them as floats.)\n",
    "            n_nodes: The max number of nodes that the DT weak classifiers are allowed to have.\n",
    "            \n",
    "        Outputs:\n",
    "            A (N, T) shaped numpy array, where T is the number of iterations / DT weak classifiers,\n",
    "            such that the t^th column contains D_{t+1} (the dataset weights at iteration t+1).\n",
    "        '''\n",
    "        #==============================================\n",
    "        # TODO: implement the fit function.\n",
    "        #==============================================   \n",
    "    pass\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict on the given dataset.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "            \n",
    "        Outputs:\n",
    "            A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "            (Even though the labels are ints, we treat them as floats.)\n",
    "        '''\n",
    "        # Initialize predictions.\n",
    "        Y_pred = np.zeros(len(X))\n",
    "        \n",
    "        # Add predictions from each DT weak classifier.\n",
    "        for i, clf in enumerate(self.clfs):\n",
    "            Y_curr = self.coefs[i] * clf.predict(X)\n",
    "            Y_pred += Y_curr\n",
    "\n",
    "        # Return the sign of the predictions.\n",
    "        return np.sign(Y_pred)\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Calculate the classification loss.\n",
    "\n",
    "        Inputs:\n",
    "            X: A (N, D) shaped numpy array containing the data points.\n",
    "            Y: A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "               (Even though the labels are ints, we treat them as floats.)\n",
    "            \n",
    "        Outputs:\n",
    "            The classification loss.\n",
    "        '''\n",
    "        # Calculate the points where the predictions and the ground truths don't match.\n",
    "        Y_pred = self.predict(X)\n",
    "        misclassified = np.where(Y_pred != Y)[0]\n",
    "\n",
    "        # Return the fraction of such points.\n",
    "        return float(len(misclassified)) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's plot the prediction results with 500 weak classifiers (misclassified points marked in black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, D = ab_suite(AdaBoost, 500, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of AdaBoost Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the training process of the AdaBoost model. First, we visualize how the predictions, as well as the dataset weights, change with each new weak classifier that we train and add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = animate_ab(model, X_train, Y_train, D, 'Training dataset predictions per iteration')\n",
    "display(HTML(anim.to_html5_video()))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize how the loss decreases with each new weak classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_loss_curves_ab(model, X_train, Y_train, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11 (main, Mar 22 2022, 16:46:40) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "51cdd729484e546d32179e58cfece8a45dfa365797bcf0655ab12c854818ccd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
